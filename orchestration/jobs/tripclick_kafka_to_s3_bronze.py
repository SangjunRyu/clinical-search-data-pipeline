from airflow import DAG
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from datetime import datetime, timedelta

DEFAULT_ARGS = {
    "owner": "data-engineer",
    "depends_on_past": False,
    "retries": 0,
    "retry_delay": timedelta(minutes=5),
}

with DAG(
    dag_id="tripclick_kafka_to_s3_bronze",
    description="Kafka TripClick logs -> S3 Bronze via Spark batch",
    default_args=DEFAULT_ARGS,
    start_date=datetime(2026, 1, 19),
    schedule_interval=None,   # ğŸ”¥ ìˆ˜ë™ ì‹¤í–‰ (ì•ˆì •í™” í›„ @daily ê°€ëŠ¥)
    catchup=False,
    tags=["tripclick", "kafka", "spark", "bronze"],
) as dag:

    kafka_to_s3_bronze = SparkSubmitOperator(
        task_id="kafka_to_s3_bronze",
        application="/opt/spark/jobs/kafka_to_s3_bronze.py",
        name="kafka-to-s3-bronze",
        conn_id="spark_default",

        # ğŸ”‘ AWS Access Key â†’ Sparkì— í™˜ê²½ë³€ìˆ˜ë¡œ ì „ë‹¬
        env_vars={
            "AWS_ACCESS_KEY_ID": "{{ conn.aws_s3_conn.login }}",
            "AWS_SECRET_ACCESS_KEY": "{{ conn.aws_s3_conn.password }}",
            "AWS_DEFAULT_REGION": "ap-northeast-2",
        },

        # Spark ì„¤ì • (í•„ìš” ì‹œ ì¶”ê°€)
        conf={
            # S3 ì ‘ê·¼ìš© (Access Key ë°©ì‹)
            "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem",
            "spark.hadoop.fs.s3a.aws.credentials.provider":
                "com.amazonaws.auth.EnvironmentVariableCredentialsProvider",

            # Kafka ì•ˆì •ì„± (ì„ íƒ)
            "spark.sql.shuffle.partitions": "4",
        },

        verbose=True,
    )

    kafka_to_s3_bronze
