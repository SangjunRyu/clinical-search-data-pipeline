# í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ batch job
# ì„¸ì…˜ ë³„ í´ë¦­ ìˆ˜, ì¼ì ë³„ ì´ë²¤íŠ¸ ìˆ˜ ì§‘ê³„

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col,
    from_json,
    to_timestamp,
    count
)
from pyspark.sql.types import (
    StructType, StructField,
    StringType, IntegerType, ArrayType
)

# -----------------------
# Spark Session
# -----------------------
spark = (
    SparkSession.builder
    .appName("TripClick-Batch-Consumer")
    .getOrCreate()
)

spark.sparkContext.setLogLevel("WARN")

# -----------------------
# Kafka batch read
# -----------------------
kafka_df = (
    spark.read
    .format("kafka")
    .option(
        "kafka.bootstrap.servers",
        "10.0.0.207:9092,10.0.0.207:9093,10.0.0.207:9094"
    )
    .option("subscribe", "tripclick_raw_logs")
    .option("startingOffsets", "earliest")
    .option("endingOffsets", "latest")
    .load()
)

print(f"ğŸ“¥ Kafka raw count: {kafka_df.count()}")

# -----------------------
# value parsing
# -----------------------
raw_df = kafka_df.selectExpr(
    "CAST(key AS STRING) as session_id",
    "CAST(value AS STRING) as json_str"
)

schema = StructType([
    StructField("DateCreated", StringType()),
    StructField("SessionId", StringType()),
    StructField("DocumentId", IntegerType()),
    StructField("Url", StringType()),
    StructField("Title", StringType()),
    StructField("DOI", StringType()),
    StructField("ClinicalAreas", StringType()),
    StructField("Keywords", StringType()),
    StructField("Documents", ArrayType(StringType())),
    StructField("event_ts", StringType()),
    StructField("event_date", StringType()),
])

parsed_df = (
    raw_df
    .withColumn("data", from_json(col("json_str"), schema))
    .select(
        col("session_id"),
        col("data.*"),
        to_timestamp(col("event_ts")).alias("event_time")
    )
)

# -----------------------
# ê¸°ë³¸ ê²€ì¦
# -----------------------
parsed_df.show(5, truncate=False)
parsed_df.printSchema()

# -----------------------
# ë°°ì¹˜ ë¶„ì„ ì˜ˆì‹œ 1
# ì„¸ì…˜ë³„ í´ë¦­ ìˆ˜
# -----------------------
session_agg_df = (
    parsed_df
    .groupBy("SessionId")
    .agg(count("*").alias("click_count"))
    .orderBy(col("click_count").desc())
)

print("ğŸ“Š Session click counts")
session_agg_df.show(10, truncate=False)

# -----------------------
# ë°°ì¹˜ ë¶„ì„ ì˜ˆì‹œ 2
# ì¼ìë³„ ì´ë²¤íŠ¸ ìˆ˜
# -----------------------
daily_agg_df = (
    parsed_df
    .groupBy("event_date")
    .agg(count("*").alias("event_count"))
    .orderBy("event_date")
)

print("ğŸ“Š Daily event counts")
daily_agg_df.show(truncate=False)

spark.stop()
