# ==============================================
# TripClick Spark Cluster
# ==============================================
# Kafka → S3 데이터 처리용 Spark 클러스터
#
# 실행:
#   docker-compose -f spark-compose.yaml up -d
#
# Spark UI: http://localhost:8181
# ==============================================

version: "3.8"

services:
  # =========================
  # Spark Master
  # =========================
  spark-master:
    build:
      context: ./spark
      dockerfile: Dockerfile
    image: tripclick-spark:latest
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      # Kafka / S3 설정 (환경변수)
      - KAFKA_BROKERS=${KAFKA_BROKERS:-localhost:9092}
      - S3_BRONZE_PATH=${S3_BRONZE_PATH:-s3a://tripclick-lake/bronze/}
      - S3_SILVER_PATH=${S3_SILVER_PATH:-s3a://tripclick-lake/silver/}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-}
    ports:
      - "7077:7077"   # Spark submit
      - "8181:8080"   # Spark Master UI
    volumes:
      - ./spark/jobs:/opt/spark/jobs
      - ./spark/config:/opt/spark/config
      - spark-checkpoint:/tmp/checkpoint
    network_mode: host
    # networks:
    #   - data-pipeline

  # =========================
  # Spark Worker 1
  # =========================
  spark-worker-1:
    image: tripclick-spark:latest
    container_name: spark-worker-1
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://localhost:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      # Kafka / S3 설정
      - KAFKA_BROKERS=${KAFKA_BROKERS:-localhost:9092}
      - S3_BRONZE_PATH=${S3_BRONZE_PATH:-s3a://tripclick-lake/bronze/}
      - S3_SILVER_PATH=${S3_SILVER_PATH:-s3a://tripclick-lake/silver/}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-}
    volumes:
      - ./spark/jobs:/opt/spark/jobs
      - ./spark/config:/opt/spark/config
      - spark-checkpoint:/tmp/checkpoint
    network_mode: host
    # networks:
    #   - data-pipeline

  # =========================
  # Spark Worker 2
  # =========================
  spark-worker-2:
    image: tripclick-spark:latest
    container_name: spark-worker-2
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://localhost:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      # Kafka / S3 설정
      - KAFKA_BROKERS=${KAFKA_BROKERS:-localhost:9092}
      - S3_BRONZE_PATH=${S3_BRONZE_PATH:-s3a://tripclick-lake/bronze/}
      - S3_SILVER_PATH=${S3_SILVER_PATH:-s3a://tripclick-lake/silver/}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID:-}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY:-}
    volumes:
      - ./spark/jobs:/opt/spark/jobs
      - ./spark/config:/opt/spark/config
      - spark-checkpoint:/tmp/checkpoint
    network_mode: host
    # networks:
    #   - data-pipeline

volumes:
  spark-checkpoint:
    driver: local

# networks:
#   data-pipeline:
#     driver: bridge
